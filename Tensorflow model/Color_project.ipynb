{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "from skimage.transform import pyramid_expand as upscale\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as lab while collecting ab distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"image_data/mit_cvcl/*.jpg\" # jpg 파일 모두 불러오기\n",
    "\n",
    "rgb_image_set = io.imread_collection(load_path)\n",
    "ab_data_mit_cvcl = np.zeros([26, 26], dtype=int)\n",
    "\n",
    "tic = time()\n",
    "for i, rgb_image in enumerate(rgb_image_set):\n",
    "    if i % 100 == 0:\n",
    "        toc = time()\n",
    "        print(\"Iter: %d / Took %.4f seconds\" % (i, toc - tic))\n",
    "        \n",
    "    lab_image = color.rgb2lab(rgb_image).astype(np.float16)\n",
    "    np.save(\"Data_preprocessed/lab_image/mit_cvcl/lab_\" + str(i), lab_image)\n",
    "\n",
    "    ab_layer = (lab_image[:, :, 1:] // 10).astype(int)\n",
    "    H, W, _ = lab_image.shape\n",
    "    for i, j in product(range(H), range(W)):\n",
    "        ab_data_mit_cvcl[tuple(ab_layer[i, j])] += 1\n",
    "        \n",
    "np.save(\"Data_preprocessed/Model_data/ab_data_mit_cvcl\", ab_data_mit_cvcl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum all ab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data_mit_cvcl = np.load(\"Data_preprocessed/Model_data/ab_data_mit_cvcl.npy\")\n",
    "ab_data_flickr = np.load(\"Data_preprocessed/Model_data/ab_data_flickr.npy\")\n",
    "\n",
    "ab_data = ab_data_mit_cvcl + ab_data_flickr\n",
    "\n",
    "np.save(\"Data_preprocessed/Model_data/ab_data\", ab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data = np.load(\"Data_preprocessed/Model_data/ab_data.npy\")\n",
    "plt.matshow(np.log(ab_data), cmap='jet')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, sigma=1):\n",
    "    return np.exp(-(x/sigma)**2 / 2) / np.sqrt(2*np.pi*sigma**2)\n",
    "\n",
    "def l2_norm(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.sqrt(np.sum((x - y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get smoothed ab distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_data = np.load(\"Data_preprocessed/Model_data/ab_data.npy\")\n",
    "filter_ab = ab_data > 0\n",
    "\n",
    "ab_distribution = np.zeros((26, 26))\n",
    "smooth_ab_data = np.zeros((26, 26))\n",
    "smooth_ab_distribution = np.zeros((26, 26))\n",
    "\n",
    "for a, b in product(range(-13, 13), repeat=2):\n",
    "    value = 0\n",
    "    for i, j in product(range(-13, 13), repeat=2):\n",
    "        g = gaussian_kernel(a - i, sigma=0.5) * gaussian_kernel(b - j, sigma=0.5)\n",
    "        value += ab_data[i, j] * g\n",
    "    smooth_ab_data[a, b] = value\n",
    "\n",
    "smooth_ab_distribution = smooth_ab_data / smooth_ab_data.sum()\n",
    "\n",
    "np.save(\"Data_preprocessed/Model_data/filter_ab\", filter_ab)\n",
    "np.save(\"Data_preprocessed/Model_data/smooth_ab_distribution\", smooth_ab_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_ab_distribution = np.load(\"Data_preprocessed/Model_data/smooth_ab_distribution.npy\")\n",
    "filter_ab = np.load(\"Data_preprocessed/Model_data/filter_ab.npy\")\n",
    "Q = np.sum(filter_ab)\n",
    "\n",
    "weight_matrix = np.zeros((26, 26))\n",
    "lam = 0.5\n",
    "\n",
    "for i, j in product(range(26), repeat=2):\n",
    "    weight_matrix[i, j] = 1 / ((1 - lam)*smooth_ab_distribution[i, j] + lam/Q)\n",
    "weight_matrix = weight_matrix * filter_ab\n",
    "weight_matrix /= np.sum(weight_matrix * smooth_ab_distribution)\n",
    "\n",
    "np.save(\"Data_preprocessed/Model_data/weight_matrix\", weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get train_X, train_y, weight_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = np.load(\"Data_preprocessed/Model_data/weight_matrix.npy\")\n",
    "filter_ab = np.load(\"Data_preprocessed/Model_data/filter_ab.npy\")\n",
    "Q = np.sum(filter_ab)\n",
    "\n",
    "tic = time()\n",
    "for n in range(2687):\n",
    "    if n % 100 == 0:\n",
    "        toc = time()\n",
    "        print(\"Iter: %d / Took %.4f seconds.\" % (n, toc - tic))\n",
    "\n",
    "    image = np.load(\"Data_preprocessed/lab_image/mit_cvcl/lab_%d.npy\" % n)\n",
    "    H, W, _ = image.shape\n",
    "    X_train = image[:, :, 0].astype(np.float16).reshape(1, H, W, 1)\n",
    "    y_train = np.zeros((1, H//4, W//4, Q), dtype=np.float16)\n",
    "    weight_train = np.zeros((1, H//4, W//4), dtype=np.float16)\n",
    "    \n",
    "    y_ab = image[:, :, 1:][2:-1:4, :, :][:, 2:-1:4, :] / 10\n",
    "    for h, w in product(range(H//4), range(W//4)):\n",
    "        distance_set = np.zeros((9, 3))\n",
    "        d = 0\n",
    "        for i, j in product(range(-1, 2), repeat=2):\n",
    "            distance_set[d] = (i, j, 10*l2_norm((i+0.5, j+0.5), y_ab[h, w] % 1))\n",
    "            d += 1\n",
    "        order = distance_set[distance_set[:, 2].argsort()]\n",
    "\n",
    "        z = np.zeros((26, 26), dtype=np.float16)\n",
    "        for i in range(5):\n",
    "            z[tuple((y_ab[h, w] + order[i, :2]).astype(int))] = gaussian_kernel(order[i, 2], sigma=5)\n",
    "        z /= np.sum(z)\n",
    "        y_train[0, h, w] = z[filter_ab]\n",
    "        \n",
    "        weight_train[0, h, w] = weight_matrix[tuple(y_ab.astype(int)[h, w])]\n",
    "    \n",
    "    np.save(\"Data_preprocessed/training_data/mit_cvcl/X_train_%d\" % n, X_train)\n",
    "    np.save(\"Data_preprocessed/training_data/mit_cvcl/y_train_%d\" % n, y_train)\n",
    "    np.save(\"Data_preprocessed/training_data/mit_cvcl/weight_train_%d\" % n, weight_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "X_train_batch = np.zeros((size, 256, 256, 1), dtype=np.float16)\n",
    "y_train_batch = np.zeros((size, 64, 64, 266), dtype=np.float16)\n",
    "weight_train_batch = np.zeros((size, 64, 64), dtype=np.float16)\n",
    "for n in range(2400):\n",
    "    i = n % size\n",
    "    X_train = np.load(\"Data_preprocessed/training_data/mit_cvcl/X_train_%d.npy\" % n)\n",
    "    y_train = np.load(\"Data_preprocessed/training_data/mit_cvcl/y_train_%d.npy\" % n)\n",
    "    weight_train = np.load(\"Data_preprocessed/training_data/mit_cvcl/weight_train_%d.npy\" % n)\n",
    "    \n",
    "    X_train_batch[i] = X_train[0]\n",
    "    y_train_batch[i] = y_train[0]\n",
    "    weight_train_batch[i] = weight_train[0]\n",
    "    \n",
    "    if i == size - 1:\n",
    "        np.save(\"Data_preprocessed/training_data/mit_cvcl_batch/X_train_%d\" % (n+1), X_train_batch)\n",
    "        np.save(\"Data_preprocessed/training_data/mit_cvcl_batch/y_train_%d\" % (n+1), y_train_batch)\n",
    "        np.save(\"Data_preprocessed/training_data/mit_cvcl_batch/weight_train_%d\" % (n+1), weight_train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Color_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_space = np.zeros((26, 26, 2), dtype=int)\n",
    "for i in range(-13, 13):\n",
    "    color_space[i, :, 0] = 10*i + 5\n",
    "    color_space[:, i, 1] = 10*i + 5\n",
    "    \n",
    "np.save(\"Data_preprocessed/Model_data/color_space\", color_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 266\n",
    "T = 0.38\n",
    "\n",
    "H, W = 256, 256\n",
    "X = tf.placeholder(tf.float32, [N, H, W, 1], name='X')\n",
    "y = tf.placeholder(tf.float32, [N, H//4, W//4, Q], name='y')\n",
    "weight = tf.placeholder(tf.float32, [N, H//4, W//4], name=\"weight\")\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def conv2d(x, W, stride=1, dilation=1, name=None):\n",
    "    strides = [1, stride, stride, 1]\n",
    "    dilations = [1, 1, dilation, dilation]\n",
    "    return tf.nn.conv2d(x, W, padding=\"SAME\", strides=strides, dilations=dilations, name=name)\n",
    "\n",
    "def conv2d_T(x, W, output_shape, stride=1, name=None):\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.conv2d_transpose(x, W, output_shape=output_shape, strides=strides, name=name)\n",
    "\n",
    "def batch_normalize(x, shape, epsilon=1e-3, name=None):\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0])\n",
    "    beta = tf.Variable(tf.zeros(shape))\n",
    "    scale = tf.Variable(tf.ones(shape))\n",
    "    return tf.nn.batch_normalization(x, batch_mean, batch_var, beta, scale, epsilon, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"conv1\"):\n",
    "    W_conv1_1 = weight_variable([3, 3, 1, 64])\n",
    "    b_conv1_1 = bias_variable([64])\n",
    "    conv1_1 = conv2d(X, W_conv1_1) + b_conv1_1\n",
    "    conv1_1 = tf.nn.relu(conv1_1)\n",
    "\n",
    "    W_conv1_2 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv1_2 = bias_variable([64])\n",
    "    conv1_2 = conv2d(conv1_1, W_conv1_2, stride=2) + b_conv1_2 # Halve size (1/2)\n",
    "    conv1_2 = tf.nn.relu(conv1_2)\n",
    "\n",
    "    conv1_norm = batch_normalize(conv1_2, [64])                                 \n",
    "\n",
    "with tf.name_scope(\"conv2\"):\n",
    "    W_conv2_1 = weight_variable([3, 3, 64, 128])\n",
    "    b_conv2_1 = bias_variable([128])\n",
    "    conv2_1 = conv2d(conv1_norm, W_conv2_1) + b_conv2_1\n",
    "    conv2_1 = tf.nn.relu(conv2_1)\n",
    "\n",
    "    W_conv2_2 = weight_variable([3, 3, 128, 128])\n",
    "    b_conv2_2 = bias_variable([128])\n",
    "    conv2_2 = conv2d(conv2_1, W_conv2_2, stride=2) + b_conv2_2 # Halve size (1/4)\n",
    "    conv2_2 = tf.nn.relu(conv2_2)\n",
    "\n",
    "    conv2_norm = batch_normalize(conv2_2, [128])\n",
    "\n",
    "with tf.name_scope(\"conv3\"):\n",
    "    W_conv3_1 = weight_variable([3, 3, 128, 256])\n",
    "    b_conv3_1 = bias_variable([256])\n",
    "    conv3_1 = conv2d(conv2_norm, W_conv3_1) + b_conv3_1\n",
    "    conv3_1 = tf.nn.relu(conv3_1)\n",
    "\n",
    "    W_conv3_2 = weight_variable([3, 3, 256, 256])\n",
    "    b_conv3_2 = bias_variable([256])\n",
    "    conv3_2 = conv2d(conv3_1, W_conv3_2) + b_conv3_2\n",
    "    conv3_2 = tf.nn.relu(conv3_2)\n",
    "\n",
    "    W_conv3_3 = weight_variable([3, 3, 256, 256])\n",
    "    b_conv3_3 = bias_variable([256])\n",
    "    conv3_3 = conv2d(conv3_2, W_conv3_3, stride=2) + b_conv3_3 # Halve size (1/8)\n",
    "    conv3_3 = tf.nn.relu(conv3_3)\n",
    "\n",
    "    conv3_norm = batch_normalize(conv3_3, [256])\n",
    "\n",
    "with tf.name_scope(\"conv4\"):\n",
    "    W_conv4_1 = weight_variable([3, 3, 256, 512])\n",
    "    b_conv4_1 = bias_variable([512])\n",
    "    conv4_1 = conv2d(conv3_norm, W_conv4_1) + b_conv4_1\n",
    "    conv4_1 = tf.nn.relu(conv4_1)\n",
    "\n",
    "    W_conv4_2 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv4_2 = bias_variable([512])\n",
    "    conv4_2 = conv2d(conv4_1, W_conv4_2) + b_conv4_2\n",
    "    conv4_2 = tf.nn.relu(conv4_2)\n",
    "\n",
    "    W_conv4_3 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv4_3 = bias_variable([512])\n",
    "    conv4_3 = conv2d(conv4_2, W_conv4_3) + b_conv4_3\n",
    "    conv4_3 = tf.nn.relu(conv4_3)\n",
    "\n",
    "    conv4_norm = batch_normalize(conv4_3, [512])\n",
    "\n",
    "with tf.name_scope(\"conv5\"):\n",
    "    W_conv5_1 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv5_1 = bias_variable([512])\n",
    "    conv5_1 = conv2d(conv4_norm, W_conv5_1, dilation=2) + b_conv5_1\n",
    "    conv5_1 = tf.nn.relu(conv5_1)\n",
    "\n",
    "    W_conv5_2 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv5_2 = bias_variable([512])\n",
    "    conv5_2 = conv2d(conv5_1, W_conv5_2, dilation=2) + b_conv5_2\n",
    "    conv5_2 = tf.nn.relu(conv5_2)\n",
    "\n",
    "    W_conv5_3 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv5_3 = bias_variable([512])\n",
    "    conv5_3 = conv2d(conv5_2, W_conv5_3, dilation=2) + b_conv5_3\n",
    "    conv5_3 = tf.nn.relu(conv5_3)\n",
    "\n",
    "    conv5_norm = batch_normalize(conv5_3, [512])\n",
    "\n",
    "with tf.name_scope(\"conv6\"):\n",
    "    W_conv6_1 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv6_1 = bias_variable([512])\n",
    "    conv6_1 = conv2d(conv5_norm, W_conv6_1, dilation=2) + b_conv6_1\n",
    "    conv6_1 = tf.nn.relu(conv6_1)\n",
    "\n",
    "    W_conv6_2 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv6_2 = bias_variable([512])\n",
    "    conv6_2 = conv2d(conv6_1, W_conv6_2, dilation=2) + b_conv6_2\n",
    "    conv6_2 = tf.nn.relu(conv6_2)\n",
    "\n",
    "    W_conv6_3 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv6_3 = bias_variable([512])\n",
    "    conv6_3 = conv2d(conv6_2, W_conv6_3, dilation=2) + b_conv6_3\n",
    "    conv6_3 = tf.nn.relu(conv6_3)\n",
    "\n",
    "    conv6_norm = batch_normalize(conv6_3, [512])\n",
    "\n",
    "with tf.name_scope(\"conv7\"):\n",
    "    W_conv7_1 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv7_1 = bias_variable([512])\n",
    "    conv7_1 = conv2d(conv6_norm, W_conv7_1) + b_conv7_1\n",
    "    conv7_1 = tf.nn.relu(conv7_1)\n",
    "\n",
    "    W_conv7_2 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv7_2 = bias_variable([512])\n",
    "    conv7_2 = conv2d(conv7_1, W_conv7_2) + b_conv7_2\n",
    "    conv7_2 = tf.nn.relu(conv7_2)\n",
    "\n",
    "    W_conv7_3 = weight_variable([3, 3, 512, 512])\n",
    "    b_conv7_3 = bias_variable([512])\n",
    "    conv7_3 = conv2d(conv7_2, W_conv7_3) + b_conv7_3\n",
    "    conv7_3 = tf.nn.relu(conv7_3)\n",
    "\n",
    "    conv7_norm = batch_normalize(conv7_3, [512])\n",
    "\n",
    "with tf.name_scope(\"deconv8\"):\n",
    "    W_conv8_1 = weight_variable([4, 4, 256, 512])\n",
    "    b_conv8_1 = bias_variable([256])\n",
    "    conv8_1 = conv2d_T(conv7_norm, W_conv8_1, output_shape=[N, H//4, W//4, 256], stride=2) + b_conv8_1 # Double size (1/4)\n",
    "    conv8_1 = tf.nn.relu(conv8_1)\n",
    "\n",
    "    W_conv8_2 = weight_variable([3, 3, 256, 256])\n",
    "    b_conv8_2 = bias_variable([256])\n",
    "    conv8_2 = conv2d(conv8_1, W_conv8_2) + b_conv8_2\n",
    "    conv8_2 = tf.nn.relu(conv8_2)\n",
    "\n",
    "    W_conv8_3 = weight_variable([3, 3, 256, 256])\n",
    "    b_conv8_3 = bias_variable([256])\n",
    "    conv8_3 = conv2d(conv8_2, W_conv8_3) + b_conv8_3\n",
    "    conv8_3 = tf.nn.relu(conv8_3)\n",
    "\n",
    "with tf.name_scope(\"Softmax\"):\n",
    "    W_conv8_Q = weight_variable([1, 1, 256, Q])\n",
    "    b_conv8_Q = bias_variable([Q])\n",
    "    conv8_Q = conv2d(conv8_3, W_conv8_Q) + b_conv8_Q\n",
    "\n",
    "    class8_Q_rh = tf.nn.softmax(conv8_Q / T, name=\"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Loss\"):\n",
    "    cross_entropy = - tf.reduce_sum(y * tf.log(class8_Q_rh + 1e-8), axis=3)\n",
    "    cost = tf.reduce_sum(weight * cross_entropy, name=\"Cost\")\n",
    "    train_step = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, \"Model/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    s = (np.random.randint(240) + 1) * 10\n",
    "    t = np.random.choice(10, N, replace=False)\n",
    "    X_train = np.load(\"Data_preprocessed/training_data/mit_cvcl_batch/mit_cvcl_X_train_%d.npy\" % s)[t]\n",
    "    y_train = np.load(\"Data_preprocessed/training_data/mit_cvcl_batch/mit_cvcl_y_train_%d.npy\" % s)[t]\n",
    "    weight_train = np.load(\"Data_preprocessed/training_data/mit_cvcl_batch/mit_cvcl_weight_train_%d.npy\" % s)[t]\n",
    "\n",
    "    print(\"Image %d start.\" % s)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(\"Model/\"))\n",
    "        tic = time()\n",
    "        for i in range(1000):\n",
    "            if i % 200 == 0:\n",
    "                toc = time()\n",
    "                print(\"Iter: %5d / Took %7.2f seconds. / Loss: %f\" % \\\n",
    "                      (i, toc - tic, sess.run(cost, feed_dict={X: X_train, y: y_train, weight: weight_train})))\n",
    "                saver.save(sess, \"Model/test\")\n",
    "            \n",
    "            if i == 950:\n",
    "                prediction = sess.run(class8_Q_rh, feed_dict={X: X_train, keep_prob: 1})\n",
    "                N, H, W, _ = prediction.shape\n",
    "\n",
    "                annealed_mean = np.sum(np.expand_dims(prediction, axis=4) * np.tile(color_space[filter_ab], (N, H, W, 1, 1)), axis=3)\n",
    "\n",
    "                image_lab = np.zeros((N, H, W, 3))\n",
    "                image_lab[:, :, :, 0] = X_train[:, 2:-1:4, :, 0][:, :, 2:-1:4]\n",
    "                image_lab[:, :, :, 1:] = annealed_mean\n",
    "\n",
    "                image_rgb = np.zeros((*X_train.shape[:3], 3), dtype=np.uint8)\n",
    "                for i in range(N):\n",
    "                    image_rgb[i] = (255*color.lab2rgb(upscale(image_lab[i], 4))).astype(np.uint8)\n",
    "\n",
    "                figure = plt.figure()\n",
    "                plt.imshow(image_rgb[0])\n",
    "                plt.show()\n",
    "\n",
    "            sess.run(train_step, feed_dict={X: X_train, y: y_train, weight: weight_train})\n",
    "    print(ctime(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
